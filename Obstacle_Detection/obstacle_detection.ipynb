{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeJUiZx02qWs"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "urllib.request.urlretrieve(\n",
        "    url='https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/main/notebooks/utils/notebook_utils.py',\n",
        "    filename='notebook_utils.py'\n",
        ")\n",
        "import collections\n",
        "import tarfile\n",
        "import time\n",
        "from pathlib import Path\n",
        "import speech_recognition as sr\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from IPython import display\n",
        "import openvino as ov\n",
        "from openvino.tools.mo.front import tf as ov_tf_front\n",
        "import speech_recognition as sr\n",
        "from openvino.tools import mo\n",
        "import notebook_utils as utils\n",
        "import pyttsx3\n",
        "\n",
        "def speakk(text):\n",
        "    engine = pyttsx3.init()\n",
        "    engine.say(text)\n",
        "    engine.runAndWait()\n",
        "\n",
        "def takeCommand():\n",
        "\n",
        "    r = sr.Recognizer()\n",
        "    with sr.Microphone() as source:\n",
        "        print(\"Listening...\")\n",
        "        r.pause_threshold = 1\n",
        "        audio = r.listen(source)\n",
        "    try:\n",
        "        print(\"Recognizing...\")\n",
        "        query = r.recognize_google(audio,language='en-in')\n",
        "        print(f\"User said :{query}\\n\")\n",
        "    except Exception as e:\n",
        "        print(\"Say that again please...\")\n",
        "        return\"None\"\n",
        "    return query\n",
        "\n",
        "\n",
        "# A directory where the model will be downloaded.\n",
        "base_model_dir = Path(\"model\")\n",
        "\n",
        "# The name of the model from Open Model Zoo\n",
        "model_name = \"ssdlite_mobilenet_v2\"\n",
        "\n",
        "archive_name = Path(f\"{model_name}_coco_2018_05_09.tar.gz\")\n",
        "model_url = f\"https://storage.openvinotoolkit.org/repositories/open_model_zoo/public/2022.1/{model_name}/{archive_name}\"\n",
        "\n",
        "# Download the archive\n",
        "downloaded_model_path = base_model_dir / archive_name\n",
        "if not downloaded_model_path.exists():\n",
        "    utils.download_file(model_url, downloaded_model_path.name, downloaded_model_path.parent)\n",
        "\n",
        "# Unpack the model\n",
        "tf_model_path = base_model_dir / archive_name.with_suffix(\"\").stem / \"frozen_inference_graph.pb\"\n",
        "if not tf_model_path.exists():\n",
        "    with tarfile.open(downloaded_model_path) as file:\n",
        "        file.extractall(base_model_dir)\n",
        "\n",
        "\n",
        "precision = \"FP16\"\n",
        "# The output path for the conversion.\n",
        "converted_model_path = Path(\"model\") / f\"{model_name}_{precision.lower()}.xml\"\n",
        "\n",
        "# Convert it to IR if not previously converted\n",
        "trans_config_path = Path(ov_tf_front.__file__).parent / \"ssd_v2_support.json\"\n",
        "if not converted_model_path.exists():\n",
        "    ov_model = mo.convert_model(\n",
        "        tf_model_path,\n",
        "        compress_to_fp16=(precision == 'FP16'),\n",
        "        transformations_config=trans_config_path,\n",
        "        tensorflow_object_detection_api_pipeline_config=tf_model_path.parent / \"pipeline.config\",\n",
        "        reverse_input_channels=True\n",
        "    )\n",
        "    ov.save_model(ov_model, converted_model_path)\n",
        "    del ov_model\n",
        "\n",
        "import ipywidgets as widgets\n",
        "\n",
        "core = ov.Core()\n",
        "\n",
        "device = widgets.Dropdown(\n",
        "    options=core.available_devices + [\"AUTO\"],\n",
        "    value='AUTO',\n",
        "    description='Device:',\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "device\n",
        "\n",
        "# Read the network and corresponding weights from a file.\n",
        "model = core.read_model(model=converted_model_path)\n",
        "# Compile the model for CPU (you can choose manually CPU, GPU etc.)\n",
        "# or let the engine choose the best available device (AUTO).\n",
        "compiled_model = core.compile_model(model=model, device_name=device.value)\n",
        "\n",
        "# Get the input and output nodes.\n",
        "input_layer = compiled_model.input(0)\n",
        "output_layer = compiled_model.output(0)\n",
        "\n",
        "# Get the input size.\n",
        "height, width = list(input_layer.shape)[1:3]\n",
        "\n",
        "# https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/\n",
        "classes = [\n",
        "    \"background\", \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\",\n",
        "    \"truck\", \"boat\", \"traffic light\", \"fire hydrant\", \"street sign\", \"stop sign\",\n",
        "    \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\",\n",
        "    \"bear\", \"zebra\", \"giraffe\", \"hat\", \"backpack\", \"umbrella\", \"shoe\", \"eye glasses\",\n",
        "    \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\",\n",
        "    \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\",\n",
        "    \"plate\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
        "    \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\",\n",
        "    \"couch\", \"potted plant\", \"bed\", \"mirror\", \"dining table\", \"window\", \"desk\", \"toilet\",\n",
        "    \"door\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\",\n",
        "    \"toaster\", \"sink\", \"refrigerator\", \"blender\", \"book\", \"clock\", \"vase\", \"scissors\",\n",
        "    \"teddy bear\", \"hair drier\", \"toothbrush\", \"hair brush\"\n",
        "]\n",
        "\n",
        "# Colors for the classes above (Rainbow Color Map).\n",
        "colors = cv2.applyColorMap(\n",
        "    src=np.arange(0, 255, 255 / len(classes), dtype=np.float32).astype(np.uint8),\n",
        "    colormap=cv2.COLORMAP_RAINBOW,\n",
        ").squeeze()\n",
        "\n",
        "def process_results(frame, results, thresh=0.6):\n",
        "    # The size of the original frame.\n",
        "    h, w = frame.shape[:2]\n",
        "    # The 'results' variable is a [1, 1, 100, 7] tensor.\n",
        "    results = results.squeeze()\n",
        "    boxes = []\n",
        "    labels = []\n",
        "    scores = []\n",
        "    for _, label, score, xmin, ymin, xmax, ymax in results:\n",
        "        # Create a box with pixels coordinates from the box with normalized coordinates [0,1].\n",
        "        boxes.append(\n",
        "            tuple(map(int, (xmin * w, ymin * h, (xmax - xmin) * w, (ymax - ymin) * h)))\n",
        "        )\n",
        "        labels.append(int(label))\n",
        "        scores.append(float(score))\n",
        "\n",
        "    # Apply non-maximum suppression to get rid of many overlapping entities.\n",
        "    # See https://paperswithcode.com/method/non-maximum-suppression\n",
        "    # This algorithm returns indices of objects to keep.\n",
        "    indices = cv2.dnn.NMSBoxes(\n",
        "        bboxes=boxes, scores=scores, score_threshold=thresh, nms_threshold=0.6\n",
        "    )\n",
        "\n",
        "    # If there are no boxes.\n",
        "    if len(indices) == 0:\n",
        "        return []\n",
        "\n",
        "    # Filter detected objects.\n",
        "    return [(labels[idx], scores[idx], boxes[idx]) for idx in indices.flatten()]\n",
        "\n",
        "def draw_boxes(frame, boxes):\n",
        "    for label, score, box in boxes:\n",
        "        # Choose color for the label.\n",
        "        color = tuple(map(int, colors[label]))\n",
        "        # Draw a box.\n",
        "        x2 = box[0] + box[2]\n",
        "        y2 = box[1] + box[3]\n",
        "        cv2.rectangle(img=frame, pt1=box[:2], pt2=(x2, y2), color=color, thickness=3)\n",
        "\n",
        "        # Draw a label name inside the box.\n",
        "        cv2.putText(\n",
        "            img=frame,\n",
        "            text=f\"{classes[label]} {score:.2f}\",\n",
        "            org=(box[0] + 10, box[1] + 30),\n",
        "            fontFace=cv2.FONT_HERSHEY_COMPLEX,\n",
        "            fontScale=frame.shape[1] / 1000,\n",
        "            color=color,\n",
        "            thickness=1,\n",
        "            lineType=cv2.LINE_AA,\n",
        "        )\n",
        "\n",
        "    return frame\n",
        "\n",
        "\n",
        "# Main processing function to run object detection.\n",
        "def run_object_detection(source=0, flip=True, use_popup=False, skip_first_frames=0):\n",
        "    player = None\n",
        "    try:\n",
        "        print(\"Starting run_object_detection function...\")\n",
        "        # Create a video player to play with target fps.\n",
        "        player = utils.VideoPlayer(\n",
        "            source=source, flip=flip, fps=30, skip_first_frames=skip_first_frames\n",
        "        )\n",
        "        print(\"Video player created.\")\n",
        "        # Start capturing.\n",
        "        player.start()\n",
        "        print(\"Video player started.\")\n",
        "        if use_popup:\n",
        "            title = \"Press ESC to Exit\"\n",
        "            cv2.namedWindow(\n",
        "                winname=title, flags=cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE\n",
        "            )\n",
        "\n",
        "        processing_times = collections.deque()\n",
        "        while True:\n",
        "            # Grab the frame.\n",
        "            frame = player.next()\n",
        "            if frame is None:\n",
        "                print(\"Source ended\")\n",
        "                break\n",
        "            # If the frame is larger than full HD, reduce size to improve the performance.\n",
        "            scale = 1280 / max(frame.shape)\n",
        "            if scale < 1:\n",
        "                frame = cv2.resize(\n",
        "                    src=frame,\n",
        "                    dsize=None,\n",
        "                    fx=scale,\n",
        "                    fy=scale,\n",
        "                    interpolation=cv2.INTER_AREA,\n",
        "                )\n",
        "\n",
        "            # Resize the image and change dims to fit neural network input.\n",
        "            input_img = cv2.resize(\n",
        "                src=frame, dsize=(width, height), interpolation=cv2.INTER_AREA\n",
        "            )\n",
        "            # Create a batch of images (size = 1).\n",
        "            input_img = input_img[np.newaxis, ...]\n",
        "\n",
        "            # Measure processing time.\n",
        "\n",
        "            start_time = time.time()\n",
        "            # Get the results.\n",
        "            results = compiled_model([input_img])[output_layer]\n",
        "            stop_time = time.time()\n",
        "            # Get poses from network results.\n",
        "            boxes = process_results(frame=frame, results=results)\n",
        "\n",
        "            # Draw boxes on a frame.\n",
        "            frame = draw_boxes(frame=frame, boxes=boxes)\n",
        "\n",
        "            # Convert detected objects to text\n",
        "            detected_objects = [classes[label] for label, _, _ in boxes]\n",
        "            detected_objects_text = \", \".join(detected_objects)\n",
        "            print(detected_objects)\n",
        "\n",
        "            processing_times.append(stop_time - start_time)\n",
        "            # Use processing times from last 200 frames.\n",
        "            if len(processing_times) > 200:\n",
        "                processing_times.popleft()\n",
        "\n",
        "            _, f_width = frame.shape[:2]\n",
        "            # Mean processing time [ms].\n",
        "            processing_time = np.mean(processing_times) * 1000\n",
        "            fps = 1000 / processing_time\n",
        "            cv2.putText(\n",
        "                img=frame,\n",
        "                text=f\"Inference time: {processing_time:.1f}ms ({fps:.1f} FPS)\",\n",
        "                org=(20, 40),\n",
        "                fontFace=cv2.FONT_HERSHEY_COMPLEX,\n",
        "                fontScale=f_width / 1000,\n",
        "                color=(0, 0, 255),\n",
        "                thickness=1,\n",
        "                lineType=cv2.LINE_AA,\n",
        "            )\n",
        "            # Speak detected objects\n",
        "            speakk(detected_objects_text+\"detected\")\n",
        "\n",
        "            # Use this workaround if there is flickering.\n",
        "            if use_popup:\n",
        "                cv2.imshow(winname=title, mat=frame)\n",
        "                key = cv2.waitKey(1)\n",
        "                # escape = 27\n",
        "                if key == 27:\n",
        "                    break\n",
        "            else:\n",
        "                # Encode numpy array to jpg.\n",
        "                _, encoded_img = cv2.imencode(\n",
        "                    ext=\".jpg\", img=frame, params=[cv2.IMWRITE_JPEG_QUALITY, 100]\n",
        "                )\n",
        "                # Create an IPython image.\n",
        "                i = display.Image(data=encoded_img)\n",
        "                # Display the image in this notebook.\n",
        "                display.clear_output(wait=True)\n",
        "                display.display(i)\n",
        "    # ctrl-c\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Interrupted\")\n",
        "    # any different error\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "    finally:\n",
        "        if player is not None:\n",
        "            # Stop capturing.\n",
        "            player.stop()\n",
        "        if use_popup:\n",
        "            cv2.destroyAllWindows()\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    while True:\n",
        "        query = takeCommand().lower()\n",
        "        if \"detect obstacle\" in query:\n",
        "            run_object_detection()"
      ]
    }
  ]
}